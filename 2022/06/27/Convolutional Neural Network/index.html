<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>Convolutional Neural Network | KYRIOTA</title>
  <meta name="author" content="KYRIOTA">
  
  <meta name="description" content="I roughly learnt CNN recently and was about to code one just like the FCNN I coded before. However this may be too costly to time, so I’ll use TensorFlow in this case and learn more about TF conveniently.">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Convolutional Neural Network"/>
  <meta property="og:site_name" content="KYRIOTA"/>

  
    <meta property="og:image" content=""/>
  

  
  
    <link href="/favicon.ico" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-70812759-1', 'auto');
  ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?cb5448498d7169c668b07c2b255d62c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="KYRIOTA" type="application/atom+xml">
</head>

 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">Home</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class=""></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class=""></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class=""></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class=""></i>About
			</a>
		  </li>
		  
		  <li>
			<a href="/friends" title="My friends.">
			  <i class=""></i>Friends
			</a>
		  </li>
		  
		  <li>
			<a href="/atom.xml" title="Subscribe me.">
			  <i class=""></i>RSS
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">
			<h1> Convolutional Neural Network</h1>
		</div>
	



<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <p>I roughly learnt CNN recently and was about to code one just like the FCNN I coded before. However this may be too costly to time, so I’ll use <code>TensorFlow</code> in this case and learn more about TF conveniently.</p>
<span id="more"></span>

<!--toc-->

<h1 id="Convolutional-Neural-Network"><a href="#Convolutional-Neural-Network" class="headerlink" title="Convolutional Neural Network"></a>Convolutional Neural Network</h1><p>(This is my first English blog, and maybe there would be more blogs in EN in the future if I’d like to practice my writing in English. My English sucks, so feel free to raise issues if any error is found)</p>
<h2 id="CNN-Introduction"><a href="#CNN-Introduction" class="headerlink" title="CNN Introduction"></a>CNN Introduction</h2><p>Actually the core feature of convolutional neural network is the convolutional layer and maybe pooling layer. But since pooling layer is usually treated as a part of convolutional layer in that it does linear calculations.</p>
<p>A convolutional layer can be concluded as following image, which I drew  by my own.</p>
<p><img src="/images/CNN_summary.png"></p>
<h3 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h3><p>A convolutional layer has two parameter matrices we need to train which are <code>Filter</code> and <code>Bias</code>. Being different from FC layer, in a convolutional layer, <code>Bias</code> matrix cannot be blended into <code>Filter</code> matrix due to the cross correlation operation.</p>
<p>Comparing with FC layers, the biggest change in convolutional layers is that convolutional layers don’t mind tensors as inputs, keeping the form of tensor from the beginning to the end. On the contrary, we would usually reshape the input to a vector at first in FCNN.</p>
<p>Mathematically, we can treat a FC layer as a particular case of convolutional layer, and the convolutional layer is the generalization of the FC layer. To prove this, just making all matrices in convolutional layer one dimension. The following image can explain the conclusion further.</p>
<p><img src="/images/CNN_CNN2FCNN.png"></p>
<h3 id="Working-Flow"><a href="#Working-Flow" class="headerlink" title="Working Flow"></a>Working Flow</h3><p>A convolutional layer feeds its input to convolutional kernels. The kernels will do cross correlation operation and add the bias accordingly. At the end, collect all outputs from all kernels and make it a new tensor by just stacking those outputs. The output of a convolutional layer is called feature map.</p>
<h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h3><p>A convolutional layer has a rather different goal from the FC layer.</p>
<p>When using FC layers, we are expecting a humanoid brain neuron like model can learn from generic inputs. However, when doing image processing, this particular topic leads to huge amount of trainable parameters in FCNN. In which case, we can do some pre-process to the input image since we can easily see that not every pixel in the input is important for the task, and this pre-process is called convolution. From this aspect, we can treat convolutional layers in the network as feature detectors, the convolutional layers learn to extract meaningful features from the image using the filter and bias tensor, and use pooling layers to compress / emphasis the feature it extracted, making fewer trainable parameters.</p>
<p>But as I mentioned, convolutional layers are only good at feature extraction. So when doing classification with multiple features, it’s still better to use FC layers. In which case, a CNN usually end up with FC layers to give the final outputs.</p>
<p>In conclusion, in CNN, the goal of a convolutional layer is extracting features from a rather huge input. The goal of a pooling layer is compressing data without damaging the main features. The goal of a FC layer in the end of CNN is to learn classification by giving features just as normal AI things would do.</p>
<h3 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h3><p>We can apply CNN in not only image recognition but also other area that shares the basic features with image recognition. For example, go and mahjong playing used CNN a lot. Especially in go playing, we can treat the go board as a 19x19 image with 48 channels (the reason why there should be 48 channels was raised by pros). and we can process every local part separately to get features of current situation. However, pooling layers are NOT used in go AI since the data loss in pooling operation is deadly to describe a go game. So design your CNN structure thoughtfully before putting it into training.</p>
<h3 id="Back-Prop"><a href="#Back-Prop" class="headerlink" title="Back Prop"></a>Back Prop</h3><p>I used to be quite curios about the back propagation in convolutional layer, and pooling layers also made me confused. But actually they are basically the same as back prop in FCNN if you expand all formulas.</p>
<p>This <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Lakz2MoHy6o&t=1097s">Video</a> explained how back prop in convolutional layer works.</p>
<p><img src="/images/CNN_BackProp.png"></p>
<p>As for max pooling, since it’s just doing linear calculation, when doing back prop, just implement the gradient to the max <code>A</code> of the previous one if there is a max pooling layer between them, and others’ gradients are 0.</p>
<h2 id="CNN-Example"><a href="#CNN-Example" class="headerlink" title="CNN Example"></a>CNN Example</h2><p>Following this <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=eMMZpas-zX0">video</a>, I coded a simple CNN example with <code>TensorFlow</code> to classify <code>CIFAR10</code>, a dataset with 10 classes as following.</p>
<blockquote>
<p>‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’</p>
</blockquote>
<p>However, in my opinion, some images are too blur to classify even for human, so it’s not surprised that AI performs poorly.</p>
<p><img src="/images/CNN_dataset.jpg"></p>
<p>Though there’s somebody made accuracy 90%+ AI with <code>ResNet</code>, that should be the content of another blog then. I made it 86.0% in 40 epochs, which is enough for experimenting CNN.</p>
<h3 id="Structure-1"><a href="#Structure-1" class="headerlink" title="Structure"></a>Structure</h3><p>Referring <code>LeNet</code>, <code>VGG</code>, and so on, we can see that the width and length of the tensor tend to go down, whereas the number of channels does increase as we go deeper into the layers of the network. Another pattern that still often repeated today is that we might have some one or more conv layers followed by a pooling layer.</p>
<p><img src="/images/CNN_LeNet.png"></p>
<center><small>LeNet</small></center>

<p>So I came up with this structure:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">Model: &quot;sequential&quot;</span><br><span class="line">_________________________________________________________________</span><br><span class="line"> Layer (type)                Output Shape              Param #   </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"> conv2d (Conv2D)             (None, 32, 32, 64)        1792      </span><br><span class="line">                                                                 </span><br><span class="line"> max_pooling2d (MaxPooling2D  (None, 16, 16, 64)       0         </span><br><span class="line"> )                                                               </span><br><span class="line">                                                                 </span><br><span class="line"> conv2d_1 (Conv2D)           (None, 16, 16, 128)       73856     </span><br><span class="line">                                                                 </span><br><span class="line"> max_pooling2d_1 (MaxPooling  (None, 8, 8, 128)        0         </span><br><span class="line"> 2D)                                                             </span><br><span class="line">                                                                 </span><br><span class="line"> conv2d_2 (Conv2D)           (None, 8, 8, 256)         295168    </span><br><span class="line">                                                                 </span><br><span class="line"> max_pooling2d_2 (MaxPooling  (None, 4, 4, 256)        0         </span><br><span class="line"> 2D)                                                             </span><br><span class="line">                                                                 </span><br><span class="line"> flatten (Flatten)           (None, 4096)              0         </span><br><span class="line">                                                                 </span><br><span class="line"> dense (Dense)               (None, 256)               1048832   </span><br><span class="line">                                                                 </span><br><span class="line"> dense_1 (Dense)             (None, 10)                2570      </span><br><span class="line">                                                                 </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 1,422,218</span><br><span class="line">Trainable params: 1,422,218</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br><span class="line">None</span><br></pre></td></tr></table></figure>

<p>However, after a few epochs, I found the accuracy too low to convince me that I have reached CNN’s full potential, which was only around 70~75%, and was heavily overfitted since the training set accuracy is around 90%.</p>
<h3 id="Training-Tricks"><a href="#Training-Tricks" class="headerlink" title="Training Tricks"></a>Training Tricks</h3><p>Actually, even though you’ve come up with a fantastic structure, or just copied a successful classic structure from Internet, without some tricky training approaches, you can barely make the network works well.</p>
<h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>Dropout is a great way to avoid overfitting. I’ve implemented this method in my FCNN project and found it useful, so I’d recommend it also to CNN. We can add dropout layer into convolutional layers or FC layers, the dropout rate should be between 0.1 and 0.5. In detail, you should make the network receive all input information or most of it, so the dropout rate of the input layer should be like 0.1 or 0, then feel free about the dropout rate in hidden layers.</p>
<p>The reason why dropout helps, in my personal belief, is that randomly disabling neurons makes your single network more likely to be made up with tons of smaller networks, which improves robustness of your model evidently.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.add(layers.Dropout(<span class="number">0.4</span>))</span><br></pre></td></tr></table></figure>

<p>The code above means it will disable 40% neurons in the previous layer randomly.</p>
<p>In CNN, it’s said adding dropout layer before or after pooling layer are both OK. I personally think adding a dropout layer after a pooling layer maximizes the effect of a dropout layer, so I’d prefer this method in most cases.</p>
<p>Adding a dropout layer will also make your validation accuracy higher than your training accuracy at the beginning. The difference between validation accuracy and training accuracy should be smaller and smaller as model learns.</p>
<h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><p>Doing regularization to weights and biases can also limit the level of overfitting because it’s minimizing the complexity of your model. This can be done by setting <code>kernel_regularizer</code> to some <code>regularizers</code> when creating a layer. The regularization parameter, or <code>weight_decay</code>, is usually set to <code>1e-4</code>.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.add(layers.Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>, kernel_regularizer=regularizers.l2(<span class="number">1e-4</span>)))</span><br></pre></td></tr></table></figure>

<h4 id="Learning-Rate-Decay"><a href="#Learning-Rate-Decay" class="headerlink" title="Learning Rate Decay"></a>Learning Rate Decay</h4><p>When we’re training the model, we’re expecting the model to learn more ‘carefully’ while it’s getting more accurate. This demand leads us to learning rate decay.</p>
<p>Firstly, we can do a rather large decay to change the stage of training manually. For example, we can start the training with <code>learning_rate=0.002</code> for 10 epochs, and reduce the learning rate to 0.001 manually for another 10 epochs, by doing which, we can make our model to learn faster and more accurate at the same time.</p>
<p>Secondly, we can do very tiny decay in every stage of training. For example, when starting with <code>learning_rate=0.002</code>, we can reduce learning rate a bit after every epoch ,usually 1e-6, which also helps. To implement this method, just set <code>decay</code> parameter when initializing your optimizer.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optim = keras.optimizers.Adam(learning_rate=<span class="number">0.0003</span>, decay=<span class="number">1e-6</span>)</span><br></pre></td></tr></table></figure>

<h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>Before feeding data to the model, we’d like to preprocess the data to make it close to Gaussian distribution to make the “landscape” of loss function smoother, in extreme cases, a model may unable to be trained or stay a low accuracy if normalization of input data is not applied. Since we can use this normalization in the input layer, we can also apply this method to hidden layers. We can collect the output of a hidden layer and make it Gaussian distribution. According to researches, doing batch normalization before and after activation are usually both OK. But we’d like to do it before activation when using sigmoid as activation function because sigmoid is happy to take input close to 0, from which we can get rather big gradient. So it’s wise to make your decision after considering the actual conditions, features of your model, and the underlying principles of the actual processes.</p>
<p>When testing the model, it would use the delta and sigma at training stage to process the testing data.</p>
<p>I forgot to implement this kind of layers at first, and got accuracy to 84.46%. After implementing batch normalization layers, the accuracy increased by 1.5%, which is an evident improvement at such accuracy. So batch normalization is really a simple and effective way to improve your network.</p>
<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>Training a ANN is very similar to alchemy. You need to tweak your network hundred times and try every little trick including but not limited to what I mentioned above to improve it. So you need enough patience and knowledge to become a really powerful AI alchemist.</p>
<h3 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h3><p>After implemented all tricks to prevent overfitting and normal optimizations such as input standardization, mini-batch, validation dataset, etc. I finally made the testing accuracy 86.03%, which is enough for me.</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        Source
    </div>
    <div class='spoiler-content'>
        <ul>
<li><p><strong>Importing</strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> regularizers</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure></li>
<li><p><strong>Preprocess</strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">dataset = keras.datasets.cifar10</span><br><span class="line">(data_images, train_labels), (test_images, test_labels) = dataset.load_data()</span><br><span class="line"></span><br><span class="line">class_names = [<span class="string">&#x27;airplane&#x27;</span>, <span class="string">&#x27;automobile&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;deer&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>]</span><br><span class="line">dataset_size = <span class="built_in">len</span>(data_images)</span><br><span class="line">train_images, test_images = data_images / <span class="number">255.0</span>, test_images / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">mean = np.mean(train_images, axis=<span class="built_in">tuple</span>(<span class="built_in">range</span>(train_images.ndim-<span class="number">1</span>)))</span><br><span class="line">std = np.std(train_images, axis=<span class="built_in">tuple</span>(<span class="built_in">range</span>(train_images.ndim-<span class="number">1</span>)))</span><br><span class="line">train_images = (train_images - mean) / std</span><br><span class="line">test_images = (test_images - mean) / std</span><br><span class="line"></span><br><span class="line">test_labels = np.squeeze(np.array(tf.one_hot(test_labels, <span class="number">10</span>)))</span><br><span class="line">train_labels = np.squeeze(np.array(tf.one_hot(train_labels, <span class="number">10</span>)))</span><br><span class="line"></span><br><span class="line">valid_images = train_images[-<span class="number">1000</span>:]</span><br><span class="line">train_images = train_images[:-<span class="number">1000</span>]</span><br><span class="line">valid_labels = train_labels[-<span class="number">1000</span>:]</span><br><span class="line">train_labels = train_labels[:-<span class="number">1000</span>]</span><br></pre></td></tr></table></figure></li>
<li><p><strong>Modeling</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">weight_decay = <span class="number">1e-4</span></span><br><span class="line"></span><br><span class="line">model = keras.models.Sequential()</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, padding=<span class="string">&#x27;same&#x27;</span>, input_shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>),</span><br><span class="line">                        kernel_regularizer=regularizers.l2(weight_decay)))</span><br><span class="line">model.add(layers.BatchNormalization())</span><br><span class="line">model.add(layers.MaxPool2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Dropout(<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(<span class="number">128</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, padding=<span class="string">&#x27;same&#x27;</span>,</span><br><span class="line">          kernel_regularizer=regularizers.l2(weight_decay)))</span><br><span class="line">model.add(layers.BatchNormalization())</span><br><span class="line">model.add(layers.MaxPool2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Dropout(<span class="number">0.4</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(<span class="number">256</span>, <span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, padding=<span class="string">&#x27;same&#x27;</span>,</span><br><span class="line">          kernel_regularizer=regularizers.l2(weight_decay)))</span><br><span class="line">model.add(layers.BatchNormalization())</span><br><span class="line">model.add(layers.MaxPool2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Dropout(<span class="number">0.4</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line"></span><br><span class="line">model.add(layers.Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>, kernel_regularizer=regularizers.l2(weight_decay)))</span><br><span class="line">model.add(layers.BatchNormalization())</span><br><span class="line">model.add(layers.Dropout(<span class="number">0.4</span>))</span><br><span class="line"></span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>, kernel_regularizer=regularizers.l2(weight_decay)))</span><br></pre></td></tr></table></figure></li>
<li><p><strong>Training</strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">loss = keras.losses.CategoricalCrossentropy()</span><br><span class="line">metrics = [<span class="string">&quot;accuracy&quot;</span>]</span><br><span class="line">history = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Train</span>(<span class="params">lr, epochs</span>):</span></span><br><span class="line">    optim = keras.optimizers.Adam(learning_rate=lr, decay=<span class="number">1e-6</span>)</span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=optim, loss=loss, metrics=metrics)</span><br><span class="line">    history.append(model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs,</span><br><span class="line">                             verbose=<span class="number">1</span>, validation_data=(valid_images, valid_labels),</span><br><span class="line">                             validation_batch_size=batch_size))</span><br><span class="line"></span><br><span class="line">Train(<span class="number">0.001</span>, <span class="number">10</span>)</span><br><span class="line">model.save(<span class="string">&#x27;Cifar10_CNN0&#x27;</span>, save_format=<span class="string">&#x27;tf&#x27;</span>)</span><br><span class="line"></span><br><span class="line">Train(<span class="number">0.0005</span>, <span class="number">10</span>)</span><br><span class="line">model.save(<span class="string">&#x27;Cifar10_CNN1&#x27;</span>, save_format=<span class="string">&#x27;tf&#x27;</span>)</span><br><span class="line"></span><br><span class="line">Train(<span class="number">0.0003</span>, <span class="number">10</span>)</span><br><span class="line">model.save(<span class="string">&#x27;Cifar10_CNN2&#x27;</span>, save_format=<span class="string">&#x27;tf&#x27;</span>)</span><br><span class="line"></span><br><span class="line">Train(<span class="number">0.0002</span>, <span class="number">10</span>)</span><br><span class="line">model.save(<span class="string">&#x27;Cifar10_CNN3&#x27;</span>, save_format=<span class="string">&#x27;tf&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>

    </div>
</div>

<h3 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h3><p><img src="/images/CNN_loss_rec.png"></p>
<center><small>Accuracy Figure</small></center>

<p><img src="/images/CNN_acc_rec.png"></p>
<center><small>Loss Figure</small></center>

<p>We can see that when changing to a smaller learning rate, the loss drops rapidly (Look at 10, 20, 30 epochs in the loss figure). Which proves that learning rate should match the stage of the model when training.</p>
<p>Finally, the most 喜闻乐见 (interesting and funny) part: Error prediction reviewing. I’ve labeled these data by <code>prediction / correct label</code></p>
<p><img src="/images/CNN_Wrong_Predictions.png"></p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>	  
	</div>

	<div>
  	<center>
	<div class="pagination">

    
    
    <a href="/2022/08/02/新建 Markdown File/" type="button" class="btn btn-default"><i
                class="fa fa-arrow-circle-o-left"></i> Prev</a>
    

    <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
    
    <a href="/2022/06/03/FriendLinksSetup/" type="button" class="btn btn-default ">Next<i
                class="fa fa-arrow-circle-o-right"></i></a>
    

    
</div>

    </center>
	</div>
	
	<!-- comment -->
	
<section id="comment">
    <h2 class="title">Comments</h2>

    
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2022-06-27 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/AI/">AI<span>4</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
		

	<!-- toc -->
	<div class="meta-widget">
	
	   <a data-toggle="collapse" data-target="#toc"><i class="fa fa-bars"></i></a>
	   <div id="toc" class="toc collapse in">
		   <span class="toc-title">Contents</span>
			<ol class="toc-article"><li class="toc-article-item toc-article-level-1"><a class="toc-article-link" href="#Convolutional-Neural-Network"><span class="toc-article-text">Convolutional Neural Network</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#CNN-Introduction"><span class="toc-article-text">CNN Introduction</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Structure"><span class="toc-article-text">Structure</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Working-Flow"><span class="toc-article-text">Working Flow</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Goal"><span class="toc-article-text">Goal</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Application"><span class="toc-article-text">Application</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Back-Prop"><span class="toc-article-text">Back Prop</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#CNN-Example"><span class="toc-article-text">CNN Example</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Structure-1"><span class="toc-article-text">Structure</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Training-Tricks"><span class="toc-article-text">Training Tricks</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#Dropout"><span class="toc-article-text">Dropout</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#Regularization"><span class="toc-article-text">Regularization</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#Learning-Rate-Decay"><span class="toc-article-text">Learning Rate Decay</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#Batch-Normalization"><span class="toc-article-text">Batch Normalization</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#Summary"><span class="toc-article-text">Summary</span></a></li></ol></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Source"><span class="toc-article-text">Source</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#Review"><span class="toc-article-text">Review</span></a></li></ol></li></ol></li></ol>
		</div>
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->



	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2022 KYRIOTA
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a>,<a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>,<a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a> and <a href="http://getbootstrap.com/" target="_blank">BOOTSTRA.386</a>. 
     <br> Theme by <a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind.386</a>.    
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>

</body>
   </html>
