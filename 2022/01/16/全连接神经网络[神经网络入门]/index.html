<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>全连接神经网络[神经网络入门] | KYRIOTA</title>
  <meta name="author" content="KYRIOTA">
  
  <meta name="description" content="全连接神经网络识别MNIST手写数字集，AI中的HelloWorld">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="全连接神经网络[神经网络入门]"/>
  <meta property="og:site_name" content="KYRIOTA"/>

  
    <meta property="og:image" content=""/>
  

  
  
    <link href="/favicon.ico" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-70812759-1', 'auto');
  ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?cb5448498d7169c668b07c2b255d62c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="KYRIOTA" type="application/atom+xml">
</head>

 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">KYRIOTA</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class=""></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class=""></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class=""></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class=""></i>About
			</a>
		  </li>
		  
		  <li>
			<a href="/atom.xml" title="Subscribe me.">
			  <i class=""></i>RSS
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">
			<h1> 全连接神经网络[神经网络入门]</h1>
		</div>
	



<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <p>全连接神经网络识别MNIST手写数字集，AI中的HelloWorld</p>
<img src="/images/FCNN_cover.png"/>

<span id="more"></span>

<!--toc-->

<h1 id="全连接神经网络-神经网络入门"><a href="#全连接神经网络-神经网络入门" class="headerlink" title="全连接神经网络[神经网络入门]"></a>全连接神经网络[神经网络入门]</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>现在的AI库诸如<code>TensorFlow</code>,<code>Keras</code>,<code>Pytorch</code>等，都可以快捷方便地在几行代码之内就构建好一个网络模型，然后开始训练等后续的事情，但把这些库当成黑盒来用的话，反正我是晚上睡不着觉的</p>
<p>虽然说全连接神经网络（Fully Connected Neural Network，以下简称<code>FCNN</code>）是人造神经网络（Artificial Neural Network，以下简称<code>ANN</code>）的入门，但在这之前也可以先做一些线性回归，逻辑回归，以及逻辑回归的多分类来加深理解和降低入门的难度，这些也是我之前做过的东西，但因为写的比较烂而且本身也不难，就不放出来了</p>
<p>因为在这篇文章之前只有<a target="_blank" rel="noopener" href="https://kyriota.com/2021/11/25/%E8%A5%BF%E6%B9%96%E8%AE%BA%E5%89%91GlobalNoise[AI%E5%AF%B9%E6%8A%97%E5%90%AF%E8%92%99]/">这篇讲对抗扰动的文章</a>是有关机器学习的，所以我也尽量在这篇文章中细🔒一些概念和理解，最好能让吃瓜的也能吃的舒服</p>
<h2 id="ANN概述"><a href="#ANN概述" class="headerlink" title="ANN概述"></a>ANN概述</h2><p>对于神经网络的认识和理解，强烈推荐<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1bx411M7Zx">3b1b的系列视频</a>（共四集），从概念到公式一条龙服务，讲的肯定比我清楚和直观</p>
<p>对于系统学习机器学习的相关知识（包括线性回归，逻辑回归等），可以看<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV164411b7dx">吴恩达系列视频</a>（这个就有点多了）</p>
<h3 id="ANN是什么"><a href="#ANN是什么" class="headerlink" title="ANN是什么"></a>ANN是什么</h3><p>神经网络不是玄学，全是数学，具体点，我目前做的东西涉及到的概念其实也就：偏导数，线性代数，链式法则，都是大一就学过的东西</p>
<p>本文会讲解如何训练一个识别MNIST手写数字集的神经网络，MNIST手写数字集是包含了数万张28x28的手写数字图像的数据集（以下简称MNIST，但其实还有MNIST衣物图片的数据集等），每个数字对应了一个<code>lable</code>，指明图像对应的数字，如下图就是一份数据集中包含的数据样本，而他对应的<code>lable</code>是<code>1</code></p>
<img src="/images/FCNN_sample.png"/>

<p>抽象的说，神经网络就是一个函数（这样思考对于CTF中构造对抗样本很有帮助），你给他一个输入，他给你一个输出</p>
<p>记这个网络为<code>h</code>（hypothesis），这个输入的<code>1</code>的数字图片样本记为<code>X</code>，则我们所期待的是模型输出一个向量来描述各个数字的概率<code>h(X)=&#123;0,1,0,0,0,0,0,0,0,0&#125;</code>（从左到右是0至9的概率，意思模型认为图片是<code>1</code>的概率是100%，是除了<code>1</code>以外的数字的概率是0%，这样的格式称为<code>OneHot Encoding</code>，好处是便于矩阵计算）</p>
<h3 id="ANN为何表现出智能"><a href="#ANN为何表现出智能" class="headerlink" title="ANN为何表现出智能"></a>ANN为何表现出智能</h3><p>在3b1b的视频中，他在Part1的5:40提到</p>
<blockquote>
<p>Why it’s even reasonable to expect a layered structure like this to behave intelligently</p>
</blockquote>
<p>他给出了一个比较让人容易接受的理解，即在多层次的结构中，上一层处理出图像的一些细微特征，然后在下一层对这些特征进行组合</p>
<p>但显然真正的网络并不是这样工作的，在3b1b的视频中，他在Part2的14:25展示了一些神经网络的权重可视化，而直观上看可视化后的权重，基本就是稍微有序一些的噪音，所以关于这个网络怎么能够得到我们所期待的功能那是这个网络自己的事情（“你已经是一个成熟的神经网络了”），只要他找到了一个还算不错的局部最优解，那其实就已经足够了</p>
<p>总结一下就是，把ANN看成一个数学模型就行，千万别想玄乎了，在生物上的神经科学发展完全之前，对“智能”的定义都会是比较模糊的，我们就算感觉AI好像拥有智能，那也只不过是一个结构比较复杂的函数</p>
<h2 id="一个神经元"><a href="#一个神经元" class="headerlink" title="一个神经元"></a>一个神经元</h2><p>我很喜欢的一句话：简单的规则可以组成复杂的系统，这句话在神经网络中同样适用，先理解单个神经元的工作原理，才能理解他们组成的网络是如何工作的，在后续对神经网络的数学推导中，我也会先对单个神经元组成的网络推导，然后再推广开来，方便我这样🧠不太好的人理解（这也是3b1b的做法，太照顾人了</p>
<p>以下内容均以下图所示网络为例</p>
<img src="/images/FCNN_cover.png"/>

<p>首先明确，FCNN是一个多层次结构，包含了输入层，隐藏层，输出层</p>
<p>单个神经元的机理很简单，观察隐藏层<code>Dense #2</code>中的第0个神经元，它也是一个函数，记作<code>Z[2][0]</code>，其中，<code>Z</code>表示神经元函数本身，<code>[2]</code>表示其在第二层，<code>[0]</code>表示其是第一个神经元，由图可知，它的输入是其上一层的所有神经元的输出，它的输出会传递到下一层的每一个神经元，这也是“全连接”的直接体现（你要是看到图里面他没有全连接起来，那多半是我抠图的时候魔棒不小心扣没了）</p>
<p>而神经元之间的信号传递其实是一个线性的过程，类比<code>y=kx+b</code>，即</p>
<center><code>Z[2][0]=∑w[2][i][0]*Z[1][i], i for i in range(0,6)</code></center><br><center><b>WARNING: 这个式子是错的，但暂时先这样理解</b></center><br>

<p>在<code>w[2][i][0]</code>中，<code>w</code>表示权重（类比k，具体点，可以把w相成是神经元之间连接的线），<code>[2]</code>表示该权重是第一层与第二层之间的，<code>[i]</code>表示上一层中的第i个，<code>[0]</code>表示下一层中的第0个（表达上有点繁琐，但并不难理解）</p>
<h3 id="偏置"><a href="#偏置" class="headerlink" title="偏置"></a>偏置</h3><p>显然相对于<code>y=kx+b</code>，还少了其中的<code>b</code>，为了补上<code>b</code>，在FCNN中，会给除了输出层之外的每一层补上一个神经元作为<code>bias</code>，又称偏置，这是一个特殊的神经元，其不接受任何输入，然后对下一层中的每一个神经元都稳定输出一个<code>+1</code>，然后下一层中偏置对每个神经元影响的大小再由对应的权重来调整，具体的连接方式如图白色部分所示</p>
<img src="/images/FCNN_bias.png"/>

<p>现在可以试着举一个例子了，比如考虑如下情况<code>?</code>的值</p>
<img src="/images/FCNN_neuronExample.png"/>

<p>而问号的下面那个神经元的值，是<code>-0.3+0.05+(-0.12)+(-0.4)=-0.77</code></p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>如果直接把线性计算得到的<code>Z</code>作为该神经元的输出传递到下一层，那么得到的网络也将是一个线性网络，这样的网络不管是十层还是百层都只能相当于一层</p>
<p>为了得到一个非线性的网络，在除了输入层之外的每一层都会在<code>Z</code>的基础上再套一个激活函数<code>A</code>，激活函数以<code>Z</code>作为输入，即神经元的输出其实是<code>A(Z)</code></p>
<p>常见的激活函数有：<code>ReLU</code>，<code>tanh</code>，<code>sigmoid</code>等，就不附图了搜一下就有（懒</p>
<p>那么，更新一以下之前提到的<code>Z</code>的式子，应该是</p>
<center><code>Z[2][0]=∑w[2][i][0]*A[2][i], i for i in range(0,6)</code></center><br>

<p>（由于偏置作为一个神经元包含进上一层的输出，就不在式子中单独加上一个<code>b</code>了）</p>
<p>对于输出层，激活函数则要根据具体问题来选择，例如MNIST手写数字集是分类0至9的数字，有多个类别，则会选用<code>softMax</code>作为输出的激活函数，其特征是累加之和为1，这符合多分类的问题的期望解，更具体的东西，比如<code>softMax</code>的求偏导，具体公式之类的，就不赘述了（懒</p>
<h3 id="流程梳理"><a href="#流程梳理" class="headerlink" title="流程梳理"></a>流程梳理</h3><p>以下梳理一下数据传递的整体流程</p>
<ul>
<li>输入的数据为<code>X</code>（输入层），直接输入到隐藏层</li>
<li>隐藏层第一层的输出为<code>A1(Z1)</code>，其中<code>Z1=w1*X</code></li>
<li>隐藏层第二层的输出为<code>A2(Z2)</code>，其中<code>Z2=w2*A1</code></li>
<li>……以此类推</li>
</ul>
<h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>主要就是梯度下降，一句话，求梯度，然后往负梯度的方向行进，可以求得一个函数的极小值，但是：为什么梯度下降这个方法可以训练一个模型，具体是怎么implement的，大概就到了大多数人的认知边界了/滑稽。考虑梯度下降之前，先了解求梯度的应用对象：损失函数</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>loss/cost function，记作<code>J</code>，是描述模型的预测结果对于真实值的差异的函数，举个例子，给了模型<code>1</code>的图片输入，模型的输出却是<code>&#123;0.5,0.5,0,0,0,0,0,0,0,0&#125;</code>，这与应该出现的结果产生了偏差，要描述这个偏差，可以直接求实际输出向量和期望的输出向量之差的L2范数，即求他们的均方误差，记期望的向量为<code>y</code>，实际输出的向量为<code>h(x)</code>，则<code>J=∑((y-h(x))*(y-h(x)))</code>，此处乘积为对应元素相乘，则计算可得loss大约是0.71</p>
<p>通过损失函数，就可以量化表示模型预测结果的准确度了，而大多数时候会使用较复杂度更高的带<code>log</code>的交叉熵函数作为损失函数，公式如下</p>
<img src="/images/FCNN_crossEntropy.png"/>

<p>看起来比较复杂，但画一个抛物线来表示均方误差函数，再画一下log的图像理解交叉熵函数就差不多了</p>
<p>交叉熵好处都有啥：收敛快，局部最优点少，知道就行</p>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>首先明确常量：训练时用的样本<code>X</code>是固定不变的，样本对应的<code>lable</code>，或记作<code>y</code>，也是不变的，唯一变化的就只有网络中间连接各个层级的权重<code>w</code>（偏置<code>b</code>包含在权重里边，因为偏置的具体大小由连接的权重控制），以及因为权重变化而跟着一起变化的预测结果，还有中间量<code>A</code>和<code>Z</code>，一切变化都来自于权重<code>w</code>的变化</p>
<p>问题现在则应该理解成：找到合适的<code>w</code>，使得<code>J</code>最小</p>
<p>具体点，就是：对<code>J(w)</code>求关于<code>w</code>的偏导，获取<code>J(w)</code>的梯度，然后更新权重<code>w</code>为<code>w-=rate*dw</code>，其中，<code>dw</code>为<code>J(w)</code>的梯度，<code>rate</code>为学习率，控制了一次下降多少，学习率过低会导致训练缓慢，学习率过高则容易各种<code>NAN</code>或者反复横跳无法收敛，需要一定的试错成本来确定合适的学习率，一般在10e-6到1之间</p>
<p>由于这部分内容网上资源挺多的，就不赘述了（懒</p>
<p>（后面会有反向传播的推导，这才是重点</p>
<h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><p>以上就是一些重要的前置（因为我怕把理解和代码融为一体会导致逻辑混乱），以下内容就正式开始涉及码代码了，坐好扶稳</p>
<h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><p>在<code>kaggle</code>下载到他们提供的MNIST手写数字数据集后，不能上手即用</p>
<ul>
<li>颜色范围0至255这个区间太大了，应该压缩到0至1，整体除以255</li>
<li>再者数据的分布也不自然，应该套一个标准分数公式，处理成正态分布</li>
</ul>
<p>关于应该对单个像素做处理还是对整体做处理（例如求均值，是对每一个像素求出对应像素位置的均值，还是整体处理，直接求出所有像素的均值），在本例中应该整体处理更好</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        Source
    </div>
    <div class='spoiler-content'>
        <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理数据集</span></span><br><span class="line">dsTrain = pd.read_csv(<span class="string">r&#x27;mnist.csv&#x27;</span>)</span><br><span class="line"><span class="comment"># 原本kaggle上的测试集是没有lable分类的，在kaggle的leaderBoard上找到正确的lable后pd.concat到原本的测试集</span></span><br><span class="line">dsTest = pd.read_csv(<span class="string">r&#x27;mnist_test.csv&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"># 获取训练集的特征</span></span><br><span class="line"><span class="string">ds_std = np.std(dsTrain.iloc[:, 1:], axis = 0) # 训练集标准差</span></span><br><span class="line"><span class="string">ds_mean = np.mean(dsTrain.iloc[:, 1:], axis = 0) # 训练集均值</span></span><br><span class="line"><span class="string"># 标准化数据，此方法是针对训练集中的同一位置的像素，效果一般，故未采用</span></span><br><span class="line"><span class="string">for i in range(len(ds.T) - 1):</span></span><br><span class="line"><span class="string">    if ds_std[i] != 0: # 边角上就很可能出现标准差为0的像素</span></span><br><span class="line"><span class="string">        dsR.iloc[:, i + 1] = (dsR.iloc[:, i + 1] - ds_mean[i]) / ds_std[i] # 标准分数公式，基本呈正态分布</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">trainX = dsTrain.iloc[:, <span class="number">1</span>:]</span><br><span class="line">trainy = dsTrain.iloc[:, :<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 排列测试集，方便在测试之后绘图</span></span><br><span class="line">testX = pd.concat([dsTest[dsTest[<span class="string">&quot;label&quot;</span>] == i].iloc[:, <span class="number">1</span>:] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)])</span><br><span class="line">testy = pd.concat([dsTest[dsTest[<span class="string">&quot;label&quot;</span>] == i].iloc[:, :<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)])</span><br><span class="line"><span class="comment"># 归一化</span></span><br><span class="line">trainX = trainX / <span class="number">255</span></span><br><span class="line">testX = testX / <span class="number">255</span></span><br><span class="line"><span class="comment"># 获取训练集特征</span></span><br><span class="line">mean = np.mean(np.array(trainX).ravel())  <span class="comment"># 训练集均值</span></span><br><span class="line">std = np.std(np.array(trainX).ravel())  <span class="comment"># 训练集标准差</span></span><br><span class="line">trainX = (trainX - mean) / std  <span class="comment"># 标准分数公式，基本呈正态分布</span></span><br><span class="line">testX = (testX - mean) / std  <span class="comment"># 用训练集的特征处理测试集数据，才能证明模型是否能识别陌生数据</span></span><br></pre></td></tr></table></figure>

    </div>
</div>

<br>

<h3 id="数学函数"><a href="#数学函数" class="headerlink" title="数学函数"></a>数学函数</h3><p>之前提到了一堆数学函数，实现起来主要就是<code>softMax</code>和<code>sigmoid</code>会比较耽误时间</p>
<p><code>leaky ReLU</code>是<code>ReLU</code>的实验性变种，通常用于在实验的时候避免因为使用<code>ReLU</code>导致大量神经元死亡，梯度消失的问题</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        Source
    </div>
    <div class='spoiler-content'>
        <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># d_func 是func的导数/偏导 derivative</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.maximum(x, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">d_relu</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x &gt; <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lrelu</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="comment"># leaky relu</span></span><br><span class="line">    <span class="keyword">return</span> np.maximum(x, <span class="number">0.1</span> * x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">d_lrelu</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (x &gt; <span class="number">0</span>) * <span class="number">0.9</span> + <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softMax</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.array(np.exp(x) / np.<span class="built_in">sum</span>(np.exp(x), axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">d_softMax</span>(<span class="params">x</span>):</span></span><br><span class="line">    x = np.matrix(softMax(x))</span><br><span class="line">    r = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> x:</span><br><span class="line">        <span class="comment"># 雅可比行列式 Jacobian</span></span><br><span class="line">        diag = np.matrix(np.diag(np.array(i)[<span class="number">0</span>]))</span><br><span class="line">        m = np.tile(i, i.shape[<span class="number">1</span>]).reshape((i.shape[<span class="number">1</span>], i.shape[<span class="number">1</span>]))</span><br><span class="line">        r.append(np.array(np.diag(diag - np.multiply(m.T, m))))</span><br><span class="line">    <span class="keyword">return</span> np.array(r)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">x</span>):</span></span><br><span class="line">    x_ravel = np.array(x).ravel()</span><br><span class="line">    length = <span class="built_in">len</span>(x_ravel)</span><br><span class="line">    y = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length):</span><br><span class="line">        <span class="keyword">if</span> x_ravel[i] &gt;= <span class="number">0</span>:</span><br><span class="line">            x_ravel[i] = <span class="built_in">min</span>(<span class="number">19</span>, x_ravel[i])  <span class="comment"># 限制范围，精度不够，np.exp()也易爆</span></span><br><span class="line">            y.append(<span class="number">1.0</span> / (<span class="number">1</span> + np.exp(-x_ravel[i])))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x_ravel[i] = <span class="built_in">max</span>(-<span class="number">744</span>, x_ravel[i])</span><br><span class="line">            y.append(np.exp(x_ravel[i]) / (np.exp(x_ravel[i]) + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> np.array(y).reshape(x.shape)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">d_sigmoid</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(x) * (<span class="number">1</span> - sigmoid(x))</span><br></pre></td></tr></table></figure>

    </div>
</div>

<br>

<h3 id="数据处理函数"><a href="#数据处理函数" class="headerlink" title="数据处理函数"></a>数据处理函数</h3><p>之前提到的<code>OneHot</code>和插入<code>bias</code></p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        Source
    </div>
    <div class='spoiler-content'>
        <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">OneHot</span>(<span class="params">length, width, y</span>):</span></span><br><span class="line">    <span class="comment"># OneHot encoding</span></span><br><span class="line">    y = np.array(y).ravel()</span><br><span class="line">    r = np.zeros((length, width))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length):</span><br><span class="line">        r[i][<span class="built_in">int</span>(y[i])] = <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> r</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">AddBias</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="comment"># 在第0列插入全为1的列</span></span><br><span class="line">    x = np.matrix(x)</span><br><span class="line">    <span class="keyword">return</span> np.c_[np.ones((x.shape[<span class="number">0</span>], <span class="number">1</span>)), x]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RemoveBias</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="comment"># 去掉第一列</span></span><br><span class="line">    <span class="keyword">return</span> np.matrix(x)[:, <span class="number">1</span>:]</span><br></pre></td></tr></table></figure>

    </div>
</div>

<br>

<h3 id="FCNN-class"><a href="#FCNN-class" class="headerlink" title="FCNN class"></a>FCNN class</h3><p>以下开始填充<code>FCNN</code>类</p>
<p>明确一下类中的基本函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FCNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Init</span>(<span class="params">self, unitNum</span>)</span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">PrepareBatch</span>(<span class="params">self, X, y, batchSize=<span class="number">0</span></span>)</span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">fit</span>(<span class="params">self, epoch, rate</span>)</span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">gradDes</span>(<span class="params">self, rate</span>)</span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">backProp</span>(<span class="params">self</span>)</span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">forwardProp</span>(<span class="params">self</span>)</span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">ClrTempResult</span>(<span class="params">self</span>)</span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">predict</span>(<span class="params">self, X, y</span>)</span></span><br></pre></td></tr></table></figure>

<p>明确一些概念</p>
<ul>
<li>batchSize：如果整个过完一遍训练集中的数据才更新一次权重，则更新速度过慢，故将数据集拆分为多批来处理，batchSize就是一批数据有多少份样本</li>
<li>epoch：总共要过几遍完整的数据，所以权重的更新次数就是<code>epoch*N/batchSize</code>，其中<code>N</code>是训练集样本总量</li>
<li>backward Propagation：后向传播，指通过梯度来调整权重的过程<strong>（重点）</strong></li>
<li>forward Propagation：前向传播，指传入样本后得到输出结果的过程</li>
</ul>
<p>以上概念虽然名词是在本文中第一次出现，但结合上文都不难理解</p>
<p>而训练过程中还有一个称作<code>DropOut</code>的优化方法，指的是在训练中随机地掐死一些神经元，前向传播的时候不经过他们，后向传播的时候也不更新他们，从而使得模型更具鲁棒性，也可以有效避免模型过拟合（过拟合：模型泛化不好，训练集和测试集的识别准确率相差过大），关于<code>DropOut</code>为什么能增强模型鲁棒性、避免过拟合的说法有很多，建议自己搜（懒</p>
<p>然后我们肯定希望模型可以保存下来，至少训练出来的权重得能保存吧，不然每次都得重来一遍</p>
<p>所以再补几个函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FCNN</span>:</span></span><br><span class="line">    <span class="comment">#...</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetDropOut</span>(<span class="params">self</span>)</span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">SaveParameters</span>(<span class="params">self</span>)</span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">LoadParameters</span>(<span class="params">self</span>)</span></span><br><span class="line"><span class="function">    #...</span></span><br></pre></td></tr></table></figure>

<p>其中<code>backProp</code>的公式推导这个重点问题放到下面的模块来说，这里则再提一些小一点的问题</p>
<ul>
<li><p>权重的初始化方法</p>
<p>  选择错误的权重初始化方法会导致梯度消失等严重问题，可能导致训练根本无法开始，由于我选择的是<code>ReLU</code>作为隐藏层的激活函数，使用了<code>He initialization</code>，具体为什么这样的初始化对<code>ReLU</code>会更友好则没有深入探究</p>
</li>
<li><p>正则项</p>
<p>  有一种防止过拟合的方法是：在损失函数中加入有关权重<code>w</code>的二次项，这样在<code>dw</code>中就体现在每次更新权重时权重都会自减一点点</p>
</li>
<li><p>DropOut的具体实现</p>
<p>  创建与<code>w</code>同样size的矩阵，将本轮中掐死的神经元的行与列置<code>0</code>，其他置<code>1</code>，传递时把<code>w</code>换成<code>np.multiply(w,dropOut)</code>即可</p>
</li>
</ul>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        Source
    </div>
    <div class='spoiler-content'>
        <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FCNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Init</span>(<span class="params">self, unitNum, dropOutProb=[]</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        完成一些初始化</span></span><br><span class="line"><span class="string">        unitNum: list 每一层的神经元数量</span></span><br><span class="line"><span class="string">        dropOutProb: list 除了输出层以外每一层的dropOut概率(0,1)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.unitNum = unitNum</span><br><span class="line">        self.layerNum = <span class="built_in">len</span>(unitNum)</span><br><span class="line">        self.dropOutProb = dropOutProb <span class="keyword">if</span> dropOutProb <span class="keyword">else</span> [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.layerNum - <span class="number">1</span>)]</span><br><span class="line">        self.outLength = unitNum[-<span class="number">1</span>]</span><br><span class="line">        self.w = [<span class="number">0</span>]  <span class="comment"># 0 的作用仅为占位</span></span><br><span class="line">        self.Z = [<span class="number">0</span>]</span><br><span class="line">        self.A = [<span class="number">0</span>]</span><br><span class="line">        self.cost = []</span><br><span class="line">        self.costTemp = []</span><br><span class="line">        self.dwNorm = []</span><br><span class="line">        self.dwNormTemp = []</span><br><span class="line">        <span class="comment"># He initialization，针对隐藏层是reLu激活函数的权重初始化方法</span></span><br><span class="line">        <span class="comment"># 正确的权重初始化方法能一定程度避免梯度消失</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, self.layerNum - <span class="number">1</span>):</span><br><span class="line">            np.random.seed(i)  <span class="comment"># 设置随机数种子</span></span><br><span class="line">            self.w.append(np.matrix(np.random.randn(unitNum[i - <span class="number">1</span>] + <span class="number">1</span>, unitNum[i])) * np.sqrt(<span class="number">2</span> / unitNum[i]))</span><br><span class="line">            self.w[-<span class="number">1</span>][<span class="number">0</span>] = np.matrix(np.zeros((<span class="number">1</span>, unitNum[i]))) <span class="comment"># bias偏置初始化为0</span></span><br><span class="line">        <span class="comment"># 最后的权重初始化和之前的不太一样，需要更加接近0，这是针对sigmoid/softMax</span></span><br><span class="line">        np.random.seed(self.layerNum - <span class="number">1</span>)</span><br><span class="line">        self.w.append(np.matrix(np.random.rand(unitNum[-<span class="number">2</span>] + <span class="number">1</span>, unitNum[-<span class="number">1</span>])))</span><br><span class="line">        self.w[-<span class="number">1</span>] = (self.w[-<span class="number">1</span>] - np.mean(self.w[-<span class="number">1</span>])) / np.std(self.w[-<span class="number">1</span>]) / <span class="number">100</span></span><br><span class="line">        self.w[-<span class="number">1</span>][<span class="number">0</span>] = np.matrix(np.zeros((<span class="number">1</span>, unitNum[-<span class="number">1</span>])))</span><br><span class="line">        self.ClrTempResult()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">PrepareBatch</span>(<span class="params">self, X, y, batchSize=<span class="number">0</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        分割训练集并完成一些初始化</span></span><br><span class="line"><span class="string">        batchSize: 每一批样本的数量，需能被样本总数整除</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        N = X.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> batchSize:</span><br><span class="line">            batchSize = N</span><br><span class="line">        <span class="keyword">if</span> N % batchSize:</span><br><span class="line">            print(<span class="string">&quot;# ERR: illegal batchSize&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        self.batchSize = batchSize</span><br><span class="line">        self.batchTime = <span class="built_in">int</span>(N / batchSize)</span><br><span class="line">        self.X = np.matrix(X)</span><br><span class="line">        self.y = OneHot(X.shape[<span class="number">0</span>], unitNum[-<span class="number">1</span>], y)</span><br><span class="line">        self.batchX = []</span><br><span class="line">        self.batchy = []</span><br><span class="line">        self.batchTime = <span class="built_in">int</span>(N / batchSize)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.batchTime):</span><br><span class="line">            self.batchX.append(np.array(self.X[i * batchSize : (i + <span class="number">1</span>) * batchSize]))</span><br><span class="line">            self.batchy.append(self.y[i * batchSize : (i + <span class="number">1</span>) * batchSize])</span><br><span class="line">            print(<span class="string">&quot;\rPreparing: %.2f%%&quot;</span> % ((i + <span class="number">1</span>) / N * batchSize * <span class="number">100</span>), end=<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, epoch, rate</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        epoch: int 训练轮次</span></span><br><span class="line"><span class="string">        rate: float 学习率</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        N = self.X.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(self.batchTime):</span><br><span class="line">                self.Z[<span class="number">0</span>] = self.batchX[j]</span><br><span class="line">                self.A[<span class="number">0</span>] = self.batchX[j]</span><br><span class="line">                self.y = self.batchy[j]</span><br><span class="line">                self.gradDes(rate)</span><br><span class="line">                self.ClrTempResult()</span><br><span class="line">                print(</span><br><span class="line">                    <span class="string">&quot;\rTotalProgress: %.2f%% BatchProgress: %.2f%%  &quot;</span></span><br><span class="line">                    % (((i + <span class="number">1</span>) / epoch * <span class="number">100</span>), ((j + <span class="number">1</span>) / N * self.batchSize * <span class="number">100</span>)),</span><br><span class="line">                    end=<span class="string">&quot;&quot;</span>,</span><br><span class="line">                )</span><br><span class="line">            <span class="comment"># 记录一个轮次下的平均损失函数</span></span><br><span class="line">            self.cost.append(<span class="built_in">sum</span>(self.costTemp) / self.batchTime)</span><br><span class="line">            self.costTemp.clear()</span><br><span class="line">            <span class="comment"># 记录一个轮次下的平均dw的L2，即模长</span></span><br><span class="line">            self.dwNorm.append(np.<span class="built_in">sum</span>(self.dwNormTemp, axis=<span class="number">0</span>) / self.batchTime)</span><br><span class="line">            self.dwNormTemp.clear()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gradDes</span>(<span class="params">self, rate</span>):</span></span><br><span class="line">        self.backProp()</span><br><span class="line">        <span class="comment"># 没有加入正则项惩罚，因为未发现过拟合现象，同时样本足够多时可以不加入正则项</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        # 一种较为简单的损失函数 J(θ)</span></span><br><span class="line"><span class="string">        J = np.sum(np.multiply(self.y-self.A[-1],self.y-self.A[-1]))/self.batchSize</span></span><br><span class="line"><span class="string">        # 这里使用更加复杂的带有log的交叉熵函数</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        J = (-np.<span class="built_in">sum</span>(np.array(self.y) * np.log(self.A[-<span class="number">1</span>]) + np.array(<span class="number">1</span> - self.y) * np.log(<span class="number">1</span> - self.A[-<span class="number">1</span>])) / self.batchSize)</span><br><span class="line">        self.costTemp.append(J)</span><br><span class="line">        dwNt = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(self.w)):</span><br><span class="line">            dwNt.append(np.linalg.norm(self.dw[i]))</span><br><span class="line">            self.w[i] -= rate * self.dw[i]</span><br><span class="line">        self.dwNormTemp.append(dwNt)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetDropOut</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 获取dropOut矩阵，根据给定概率随机去掉神经元</span></span><br><span class="line">        index = []</span><br><span class="line">        prob = self.dropOutProb[<span class="number">0</span>]</span><br><span class="line">        num = <span class="number">0</span> <span class="keyword">if</span> <span class="keyword">not</span> prob <span class="keyword">else</span> <span class="built_in">int</span>(np.clip(np.random.randn() / <span class="number">50</span> + prob, <span class="number">0</span>, <span class="number">0.75</span>) * self.unitNum[<span class="number">0</span>])</span><br><span class="line">        index.append(np.random.choice(self.unitNum[<span class="number">0</span>], num, <span class="literal">False</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, self.layerNum - <span class="number">1</span>):</span><br><span class="line">            prob = self.dropOutProb[i]</span><br><span class="line">            num = <span class="number">0</span> <span class="keyword">if</span> <span class="keyword">not</span> prob <span class="keyword">else</span> <span class="built_in">int</span>(np.clip(np.random.randn() / <span class="number">50</span> + prob, <span class="number">0</span>, <span class="number">0.75</span>) * self.unitNum[i])</span><br><span class="line">            index.append(np.random.choice(self.unitNum[i], num, <span class="literal">False</span>))</span><br><span class="line">            dropOutMat = np.ones((self.unitNum[i - <span class="number">1</span>], self.unitNum[i]))</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> index[i - <span class="number">1</span>]:</span><br><span class="line">                dropOutMat[j] = np.zeros((<span class="number">1</span>, self.unitNum[i]))</span><br><span class="line">            dropOutMat = np.transpose(dropOutMat)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> index[i]:</span><br><span class="line">                dropOutMat[j] = np.zeros((<span class="number">1</span>, self.unitNum[i - <span class="number">1</span>]))</span><br><span class="line">            dropOutMat = np.r_[np.ones((<span class="number">1</span>, self.unitNum[i])), np.transpose(dropOutMat)]</span><br><span class="line">            self.dropOut.append(np.matrix(dropOutMat))</span><br><span class="line">        dropOutMat = np.ones((self.unitNum[-<span class="number">2</span>], self.unitNum[-<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> index[-<span class="number">1</span>]:</span><br><span class="line">            dropOutMat[j] = np.zeros((<span class="number">1</span>, self.unitNum[-<span class="number">1</span>]))</span><br><span class="line">        dropOutMat = np.r_[np.ones((<span class="number">1</span>, self.unitNum[-<span class="number">1</span>])), dropOutMat]</span><br><span class="line">        self.dropOut.append(np.matrix(dropOutMat))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backProp</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 有详细推导过程</span></span><br><span class="line">        self.forwardProp()</span><br><span class="line">        self.dA[-<span class="number">1</span>] = np.matrix(self.A[-<span class="number">1</span>] - self.y)</span><br><span class="line">        self.dZ[-<span class="number">1</span>] = np.multiply(d_softMax(self.Z[-<span class="number">1</span>]), self.dA[-<span class="number">1</span>])</span><br><span class="line">        dw = self.A[-<span class="number">2</span>].T * self.dZ[-<span class="number">1</span>]</span><br><span class="line">        db = np.<span class="built_in">sum</span>(self.dZ[-<span class="number">1</span>], axis=<span class="number">0</span>)</span><br><span class="line">        self.dw[-<span class="number">1</span>] = np.r_[db, dw] / self.batchSize</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="number">1</span>, self.layerNum - <span class="number">1</span>)):</span><br><span class="line">            self.dA[i] = RemoveBias(self.dZ[i + <span class="number">1</span>] * np.multiply(self.w[i + <span class="number">1</span>], self.dropOut[i]).T)</span><br><span class="line">            self.dZ[i] = np.multiply(d_relu(self.Z[i]), self.dA[i])</span><br><span class="line">            dw = self.A[i - <span class="number">1</span>].T * self.dZ[i]</span><br><span class="line">            db = np.<span class="built_in">sum</span>(self.dZ[i], axis=<span class="number">0</span>)</span><br><span class="line">            self.dw[i] = np.multiply(np.r_[db, dw] / self.batchSize, self.dropOut[i - <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forwardProp</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.GetDropOut()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, self.layerNum - <span class="number">1</span>):</span><br><span class="line">            self.Z.append(AddBias(self.A[-<span class="number">1</span>]) * np.multiply(self.w[i], self.dropOut[i - <span class="number">1</span>]))</span><br><span class="line">            self.A.append(relu(self.Z[-<span class="number">1</span>]))</span><br><span class="line">        self.Z.append(AddBias(self.A[-<span class="number">1</span>]) * np.multiply(self.w[-<span class="number">1</span>], self.dropOut[-<span class="number">1</span>]))</span><br><span class="line">        self.A.append(softMax(self.Z[-<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ClrTempResult</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.Z = self.Z[<span class="number">0</span>:<span class="number">1</span>]</span><br><span class="line">        self.A = self.A[<span class="number">0</span>:<span class="number">1</span>]</span><br><span class="line">        self.dw = <span class="built_in">list</span>(np.repeat(<span class="number">1</span>, self.layerNum, axis=<span class="number">0</span>))</span><br><span class="line">        self.dZ = <span class="built_in">list</span>(np.repeat(<span class="number">1</span>, self.layerNum, axis=<span class="number">0</span>))</span><br><span class="line">        self.dA = <span class="built_in">list</span>(np.repeat(<span class="number">1</span>, self.layerNum, axis=<span class="number">0</span>))</span><br><span class="line">        self.dropOut = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, X, y</span>):</span></span><br><span class="line">        self.ClrTempResult()</span><br><span class="line">        self.dropOutProb = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.layerNum - <span class="number">1</span>)]</span><br><span class="line">        self.Z[<span class="number">0</span>] = np.matrix(X)</span><br><span class="line">        self.A[<span class="number">0</span>] = np.matrix(X)</span><br><span class="line">        self.forwardProp()</span><br><span class="line">        r_p = self.A[-<span class="number">1</span>]</span><br><span class="line">        self.r = np.zeros((r_p.shape[<span class="number">0</span>], <span class="number">1</span>))</span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        self.wrong = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(r_p.shape[<span class="number">0</span>]):</span><br><span class="line">            self.r[i] = np.argmax(r_p[i])</span><br><span class="line">            <span class="keyword">if</span> self.r[i] == np.array(y).ravel()[i]:</span><br><span class="line">                correct += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.wrong.append(i) <span class="comment"># 储存错误样本的index</span></span><br><span class="line">        print(<span class="string">&quot;correct: &quot;</span> + <span class="built_in">str</span>(correct / r_p.shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SaveParameters</span>(<span class="params">self</span>):</span></span><br><span class="line">        f = <span class="built_in">open</span>(<span class="string">&quot;FCNNweight.txt&quot;</span>, <span class="string">&quot;wb&quot;</span>)</span><br><span class="line">        wt = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> self.w[<span class="number">1</span>:]:</span><br><span class="line">            wt.extend(<span class="built_in">list</span>(np.array(i).ravel()))</span><br><span class="line">        f.write(np.array(wt).tobytes())</span><br><span class="line">        f.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">LoadParameters</span>(<span class="params">self</span>):</span></span><br><span class="line">        f = <span class="built_in">open</span>(<span class="string">&quot;FCNNweight.txt&quot;</span>, <span class="string">&quot;rb&quot;</span>)</span><br><span class="line">        wt = np.frombuffer(f.read(), np.float64)</span><br><span class="line">        f.close()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(self.unitNum)):</span><br><span class="line">            self.w[i] = np.matrix(wt[: (self.unitNum[i - <span class="number">1</span>] + <span class="number">1</span>) * self.unitNum[i]]).reshape((self.unitNum[i - <span class="number">1</span>] + <span class="number">1</span>, self.unitNum[i]))</span><br><span class="line">            wt = wt[(self.unitNum[i - <span class="number">1</span>] + <span class="number">1</span>) * self.unitNum[i] :]</span><br></pre></td></tr></table></figure>

    </div>
</div>

<br>

<h3 id="后向传播推导"><a href="#后向传播推导" class="headerlink" title="后向传播推导"></a>后向传播推导</h3><p>以下为我梳理代码的时候顺便写的过程</p>
<p>链式法则推单个神经元，然后扩展到矩阵</p>
<p>强烈建议搭配3b1b系列part4（或者是part3[下]？）食用，附上一张3b1b视频的截图</p>
<img src="/images/FCNN_formula.png"/>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">layerNum = <span class="number">4</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> input -&gt; hidden 1 -&gt; hidden 2 -&gt; output</span></span><br><span class="line"><span class="string"> 784+1 -&gt;   H1+1   -&gt;   H2+1   -&gt;   10</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> Z0(X)  ┌───▶Z1     ┌───▶Z2     ┌──▶Z3</span></span><br><span class="line"><span class="string">   │    │     ↓     │     ↓     │    ↓</span></span><br><span class="line"><span class="string">   │  [w1] [relu] [w2] [relu] [w3][softmax]</span></span><br><span class="line"><span class="string">   ↓    │     ↓     │     ↓     │    ↓</span></span><br><span class="line"><span class="string"> A0(X)──┘    A1─────┘    A2─────┘   A3</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">unityNum = [<span class="number">784</span>, H1, H2, <span class="number">10</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">### Init ###</span></span><br><span class="line">w = [ <span class="number">0</span>, [w1(<span class="number">784</span>+<span class="number">1</span>,H1)], [w2(H1+<span class="number">1</span>,H2)], [w3(H2+<span class="number">1</span>,<span class="number">10</span>)] ]</span><br><span class="line">Z = [ [X(N,<span class="number">784</span>)] ]</span><br><span class="line"><span class="comment"># A has same structure as Z</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### forwardProp ###</span></span><br><span class="line">Z = [ [X(N,<span class="number">784</span>)], [Z1(N,H1)], [Z2(N,H2)], [Z3(N,<span class="number">10</span>)] ]</span><br><span class="line"><span class="comment"># A has same structure as Z</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### backProp ###</span></span><br><span class="line"><span class="comment"># dX means δJ/δX</span></span><br><span class="line"><span class="comment"># aka partial derivative of parameter X with respect to cost function J</span></span><br><span class="line"><span class="comment"># to get formulas as below, apply chain rule</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"># u[i] indicates i.th layer of neural network</span></span><br><span class="line"><span class="string"># and each layer has only 1 neuron</span></span><br><span class="line"><span class="string"># L.th layer indicates output layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ....    &#123;u[L-1]&#125;────&#123;u[L]&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># Deduction :</span></span><br><span class="line"><span class="string">- layer L :</span></span><br><span class="line"><span class="string">    dA[L] = A[L] - y</span></span><br><span class="line"><span class="string">    dZ[L] = δA[L]/δZ[L] * dA[L] = sigmoidDeriv(Z[L]) * dA[L]</span></span><br><span class="line"><span class="string">    dw[L] = δw[L]/δZ[L] * dZ[L] = A[L-1] * dZ[L]</span></span><br><span class="line"><span class="string">    db[L] = δb[L]/δZ[L] * dZ[L] = dZ[L]</span></span><br><span class="line"><span class="string">- layer L-1 :</span></span><br><span class="line"><span class="string">    dA[L-1] = δZ[L]/δA[L-1] * dZ[L] = w[L] * dZ[L]</span></span><br><span class="line"><span class="string">    dZ[L-1] = δA[L-1]/δZ[L-1] * dA[L-1] = reluDeriv(Z[L-1]) * dA[L-1]</span></span><br><span class="line"><span class="string">    ... etc.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># extra process making formulas can be applied to matrices or layers</span></span><br><span class="line"><span class="comment"># no dot product is applied below, just multiplying elements by elements</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"># when each layer has more than 1 neuron</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            &#123;u[L-1][0]&#125;</span></span><br><span class="line"><span class="string">                         ┌─&#123;u[L][0]&#125;</span></span><br><span class="line"><span class="string">    ....    &#123;u[L-1][1]&#125;══╡</span></span><br><span class="line"><span class="string">                         └─&#123;u[L][1]&#125;</span></span><br><span class="line"><span class="string">            &#123;u[L-1][2]&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># for a certain neuron in hidden layer, e.g. as for u[L-1][1]</span></span><br><span class="line"><span class="string"># w[L][1][0] indicates weight between &#123;u[L-1][1]&#125; and &#123;u[L][0]&#125;</span></span><br><span class="line"><span class="string"># Deduction :</span></span><br><span class="line"><span class="string">- layer L :</span></span><br><span class="line"><span class="string">    dA[L][0] = A[L][0] - y[0]</span></span><br><span class="line"><span class="string">    dA[L][1] = A[L][1] - y[1]</span></span><br><span class="line"><span class="string">    dZ[L][0] = sigmoidDeriv(Z[L][0]) * dA[L][0]</span></span><br><span class="line"><span class="string">    dZ[L][1] = sigmoidDeriv(Z[L][1]) * dA[L][1]</span></span><br><span class="line"><span class="string">    dw[L][1][0] = A[L-1][1] * dZ[L][0]</span></span><br><span class="line"><span class="string">    dw[L][1][1] = A[L-1][1] * dZ[L][1]</span></span><br><span class="line"><span class="string">    db[L][1][0] = dZ[L][0]</span></span><br><span class="line"><span class="string">    db[L][1][1] = dZ[L][1]</span></span><br><span class="line"><span class="string">- layer L-1 :</span></span><br><span class="line"><span class="string">    dA[L-1][1] = w[L][1][0] * dZ[L][0] + w[L][1][1] * dZ[L][1]</span></span><br><span class="line"><span class="string">    dZ[L-1][1] = reluDeriv(Z[L-1][1]) * dA[L-1][1]</span></span><br><span class="line"><span class="string">    ... etc</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># e.g. as for 3.rd layer, which has index of 2 :</span></span><br><span class="line"><span class="string">    dA2(N,H2) = RemoveBias(dZ3(N,10) * w3.T(10,H2+1))</span></span><br><span class="line"><span class="string">    dZ2(N,H2) = np.multiply(reluDeriv(Z2)(N,H2),dA2(N,H2))</span></span><br><span class="line"><span class="string">    dw2(H1,H2) = A1.T(H1,N) * dZ2(N,H2)</span></span><br><span class="line"><span class="string">    db2(1,H2) = np.sum(dZ2(N,H2),axis=0)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">dA = [ <span class="number">1</span>, [dA1(N,H1)], [dA2(N,H2)], [dA3(N,<span class="number">10</span>)] ]</span><br><span class="line">dw = [ <span class="number">1</span>, [dw0(<span class="number">784</span>+<span class="number">1</span>,H1)], [dw1(H1+<span class="number">1</span>,H2)], [dw2(H2+<span class="number">1</span>,<span class="number">10</span>)] ]</span><br></pre></td></tr></table></figure>

<h3 id="main"><a href="#main" class="headerlink" title="main"></a>main</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">unitNum = [<span class="number">784</span>, <span class="number">512</span>, <span class="number">256</span>, <span class="number">10</span>]</span><br><span class="line">dropOutProb = [<span class="number">0.4</span>, <span class="number">0.2</span>, <span class="number">0.1</span>]</span><br><span class="line">batchSize = <span class="number">120</span></span><br><span class="line">epoch = <span class="number">50</span></span><br><span class="line">rate = <span class="number">0.08</span></span><br><span class="line"></span><br><span class="line">FCNN = FCNN()</span><br><span class="line">FCNN.Init(unitNum, dropOutProb)</span><br><span class="line">FCNN.PrepareBatch(trainX, trainy, batchSize)</span><br><span class="line">FCNN.fit(epoch, rate)</span><br><span class="line">FCNN.SaveParameters()</span><br></pre></td></tr></table></figure>

<h3 id="后续验证"><a href="#后续验证" class="headerlink" title="后续验证"></a>后续验证</h3><p>在训练完只会应当绘制<code>loss</code>图像，用于确保梯度下降是没有问题的（<code>dw</code>模长单纯是我想看一下而已）</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        Source
    </div>
    <div class='spoiler-content'>
        <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ShowCostFig</span>(<span class="params">epoch, cost</span>):</span></span><br><span class="line">    <span class="comment"># 绘制损失函数在迭代中的变化图像</span></span><br><span class="line">    ax = plt.figure().gca()</span><br><span class="line">    ax.xaxis.set_major_locator(MaxNLocator(integer=<span class="literal">True</span>))  <span class="comment"># 使坐标刻度为整数</span></span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="number">0</span>, epoch), cost[<span class="number">0</span>:], <span class="string">&quot;go-&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">&quot;Final Cost: &quot;</span> + <span class="built_in">str</span>(cost[epoch - <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ShowdwFig</span>(<span class="params">epoch, dw, layerNum</span>):</span></span><br><span class="line">    <span class="comment"># 绘制dw的L2在迭代中的变化图像</span></span><br><span class="line">    camp = getCmap(layerNum)</span><br><span class="line">    ax = plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>)).gca()</span><br><span class="line">    ax.xaxis.set_major_locator(MaxNLocator(integer=<span class="literal">True</span>))</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(np.transpose(dw)):</span><br><span class="line">        plt.plot(data[<span class="number">1</span>:], c=camp(i), label=i)</span><br><span class="line">    plt.legend()  <span class="comment"># 加上图例</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getCmap</span>(<span class="params">n, name=<span class="string">&quot;hsv&quot;</span></span>):</span></span><br><span class="line">    <span class="comment"># 从网上copy了一个调色盘来选颜色</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns a function that maps each index in 0, 1, ..., n-1 to a distinct</span></span><br><span class="line"><span class="string">    RGB color; the keyword argument name must be a standard mpl colormap name.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> plt.cm.get_cmap(name, n)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ShowNorm</span>(<span class="params">x</span>):</span></span><br><span class="line">    print(<span class="string">&quot;Norm: &quot;</span> + <span class="built_in">str</span>(np.linalg.norm(x)))</span><br></pre></td></tr></table></figure>

    </div>
</div>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">FCNN.predict(trainX, trainy)</span><br><span class="line">&gt;&gt; correct: <span class="number">0.9697380952380953</span></span><br><span class="line">FCNN.predict(testX, testy)</span><br><span class="line">&gt;&gt; correct: <span class="number">0.9618928571428571</span></span><br><span class="line">ShowNorm(FCNN.w[-<span class="number">1</span>])</span><br><span class="line">&gt;&gt; Norm: <span class="number">4.718137923777051</span></span><br><span class="line">ShowCostFig(epoch, FCNN.cost)</span><br><span class="line">&gt;&gt; Final Cost: <span class="number">0.3456541492963639</span></span><br><span class="line">ShowdwFig(epoch, FCNN.dwNorm, FCNN.layerNum)</span><br></pre></td></tr></table></figure>

<img src="/images/FCNN_costFig.png"/>

<img src="/images/FCNN_dwFig.png"/>

<p>可见<code>loss</code>的确在以肉眼可见速度收敛，且模型对训练集和测试集的预测准确度相差不大，都保持在96%~97%，对我来说我已经满意了</p>
<p>最后当然是喜闻乐见的错误样本抽样（只能说相当一部分的错误样本也都不是什么善茬</p>
<img src="/images/FCNN_wrong.png"/>

<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>	  
	</div>

	<div>
  	<center>
	<div class="pagination">

    
    
    <a type="button" class="btn btn-default disabled"><i class="fa fa-arrow-circle-o-left"></i>Prev</a>
    

    <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
    
    <a href="/2022/01/05/数据结构大作业/" type="button" class="btn btn-default ">Next<i
                class="fa fa-arrow-circle-o-right"></i></a>
    

    
</div>

    </center>
	</div>
	
	<!-- comment -->
	
<section id="comment">
    <h2 class="title">Comments</h2>

    
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2022-01-16 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/AI/">AI<span>2</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
		

	<!-- toc -->
	<div class="meta-widget">
	
	   <a data-toggle="collapse" data-target="#toc"><i class="fa fa-bars"></i></a>
	   <div id="toc" class="toc collapse in">
		   <span class="toc-title">Contents</span>
			<ol class="toc-article"><li class="toc-article-item toc-article-level-1"><a class="toc-article-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8"><span class="toc-article-text">全连接神经网络[神经网络入门]</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-article-text">前言</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#ANN%E6%A6%82%E8%BF%B0"><span class="toc-article-text">ANN概述</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#ANN%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-article-text">ANN是什么</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#ANN%E4%B8%BA%E4%BD%95%E8%A1%A8%E7%8E%B0%E5%87%BA%E6%99%BA%E8%83%BD"><span class="toc-article-text">ANN为何表现出智能</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E5%85%83"><span class="toc-article-text">一个神经元</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E5%81%8F%E7%BD%AE"><span class="toc-article-text">偏置</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-article-text">激活函数</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E6%B5%81%E7%A8%8B%E6%A2%B3%E7%90%86"><span class="toc-article-text">流程梳理</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-article-text">训练过程</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-article-text">损失函数</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-article-text">梯度下降</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0"><span class="toc-article-text">具体实现</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-article-text">预处理</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E6%95%B0%E5%AD%A6%E5%87%BD%E6%95%B0"><span class="toc-article-text">数学函数</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%87%BD%E6%95%B0"><span class="toc-article-text">数据处理函数</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#FCNN-class"><span class="toc-article-text">FCNN class</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E5%90%8E%E5%90%91%E4%BC%A0%E6%92%AD%E6%8E%A8%E5%AF%BC"><span class="toc-article-text">后向传播推导</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#main"><span class="toc-article-text">main</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E5%90%8E%E7%BB%AD%E9%AA%8C%E8%AF%81"><span class="toc-article-text">后续验证</span></a></li></ol></li></ol></li></ol>
		</div>
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->



	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2022 KYRIOTA
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a>,<a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>,<a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a> and <a href="http://getbootstrap.com/" target="_blank">BOOTSTRA.386</a>. 
     <br> Theme by <a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind.386</a>.    
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>

</body>
   </html>
